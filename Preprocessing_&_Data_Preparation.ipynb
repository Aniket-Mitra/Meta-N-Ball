{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGyP6PU+kKQpggoCF3zykR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniket-Mitra/Meta-N-Ball/blob/main/Preprocessing_%26_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Metadata-Info (Synonyms)"
      ],
      "metadata": {
        "id": "fG3SNL0GpxwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRjX7kn8pXz9"
      },
      "outputs": [],
      "source": [
        "list_line2=[]\n",
        "with open(\"hp.obo\") as f:  #we will extract metadata info from .obo file\n",
        "        for k,line in enumerate(f):\n",
        "          x = line[:-1]\n",
        "          list_line2.append(x)\n",
        "#list_line2[-1]=list_line2[-1]+')'\n",
        "#list_line2[-1]\n",
        "list_line2[-2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_metaaxioms(type1='EXACT'):\n",
        "  if type1=='EXACT [':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasExactSynonym'\n",
        "  elif type1=='NARROW [':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym'\n",
        "  elif type1=='BROAD [':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym'\n",
        "  elif type1=='RELATED [':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym'\n",
        "  dict_all=[]\n",
        "  count=-1\n",
        "  for i in list_line2:\n",
        "   count+=1\n",
        "   if type1 in i:\n",
        "    exact_syn=i.split('\"')\n",
        "    for ij in range(count,0,-1):\n",
        "          if list_line2[ij].startswith('id:'):\n",
        "            id=list_line2[ij].split(' ')[1]\n",
        "            id=id.replace(':','_')\n",
        "            break\n",
        "    axioms1='http://purl.obolibrary.org/obo/'+id+' '+url+' '+exact_syn[1]\n",
        "    dict_all.append(axioms1)\n",
        "  return dict_all"
      ],
      "metadata": {
        "id": "xsSBU1RGptqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_all_exact=create_metaaxioms(type1='EXACT [')\n",
        "dict_all_exact=list(set(dict_all_exact))\n",
        "print(dict_all_exact[-1])\n",
        "print(len(dict_all_exact))"
      ],
      "metadata": {
        "id": "TbQG88-qqoe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_all_narrow=create_metaaxioms(type1='NARROW [')\n",
        "dict_all_narrow=list(set(dict_all_narrow))\n",
        "print(dict_all_narrow[-1])\n",
        "print(len(dict_all_narrow))"
      ],
      "metadata": {
        "id": "YNPjEuozqrWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_all_broad=create_metaaxioms(type1='BROAD [') # run this for all the 4 types of synonyms- BROAD, EXACT, RELATED, NARROW\n",
        "dict_all_broad=list(set(dict_all_broad))\n",
        "print(dict_all_broad[-1])\n",
        "print(len(dict_all_broad))"
      ],
      "metadata": {
        "id": "PXkIgIEzqtve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_annots=dict_all_exact+dict_all_narrow+dict_all_broad+dict_all_related # append the list of all the types of synonyms obtained"
      ],
      "metadata": {
        "id": "VEeuR3ahq4rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts='\\n'.join(dict_annots)\n",
        "file = open('annotations_code2_hp.txt', 'w') # the extracted synonym information is stored in this file\n",
        "file.write(ts)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "CPLLskEarD6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata-Term Extraction NLP Model"
      ],
      "metadata": {
        "id": "1LXnd1sTrKEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod3=[]\n",
        "with open(r'annotations_code2_hp.txt', 'r') as fp:\n",
        "    for line in fp:\n",
        "        # remove linebreak from a current name\n",
        "        # linebreak is the last character of each line\n",
        "        #if '0002481' in line :\n",
        "           x = line[:-1]\n",
        "\n",
        "        # add current item to the list\n",
        "           mod3.append(x)"
      ],
      "metadata": {
        "id": "iCdKKk-BrS6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_axioms(type1,index_length):\n",
        "   if type1=='hasExactSynonym':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasExactSynonym'\n",
        "   elif type1=='hasNarrowSynonym':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym'\n",
        "   elif type1=='hasBroadSynonym':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym'\n",
        "   elif type1=='hasRelatedSynonym':\n",
        "    url='http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym'\n",
        "\n",
        "   narrow_axioms=[]\n",
        "   narrow_list={}\n",
        "   for line in mod3:\n",
        "      if type1 in line and 'rdf-schema#label' not in line:\n",
        "    #print(i)\n",
        "          def_index=line.index(url)\n",
        "          class_name=line[:def_index-1]\n",
        "          defs=line[def_index*2+index_length:]\n",
        "    #defs=line[def_index*2+21:]\n",
        "          defs=defs.replace(' ','_')\n",
        "    #narrow_list.append(class_name+defs)\n",
        "          if defs not in narrow_list:\n",
        "               narrow_list[defs]=[class_name]\n",
        "          else:\n",
        "               narrow_list[defs].append(class_name)\n",
        "   return narrow_list\n",
        "\n",
        "#narrow_list=get_axioms(type1='hasNarrowSynonym',index_length=20)\n",
        "#narrow_list=get_axioms(type1='hasBroadSynonym',index_length=19)\n",
        "#narrow_list=get_axioms(type1='hasRelatedSynonym',index_length=21)\n",
        "narrow_list=get_axioms(type1='hasExactSynonym',index_length=19)"
      ],
      "metadata": {
        "id": "ErXekVa2rZQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy\n",
        "\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_md-0.5.3.tar.gz"
      ],
      "metadata": {
        "id": "of2F76mYrdpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in narrow_list:\n",
        "  if len(narrow_list[i])>1:\n",
        "     print(narrow_list[i],i)\n",
        "     narrow_list[i]=[' '.join(narrow_list[i]) ]\n",
        "\n",
        "import pandas as pd\n",
        "df3=pd.DataFrame(narrow_list).T.reset_index()\n",
        "df3.rename(columns={\"index\":'metadata',0:'url'},inplace=True)\n",
        "df3.head()"
      ],
      "metadata": {
        "id": "F2VCF9Kqrg4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df2=pd.DataFrame()\n",
        "df2=pd.concat([df2,df3])\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "VcQHb87KrpEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['metadata']=df2.metadata.str.replace('_',' ').str.lower()\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "-a5WbfrRs5qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2temp=df2.copy()"
      ],
      "metadata": {
        "id": "v8tFhK_9s6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNS9Uy6p_AOC",
        "outputId": "4b7dde3e-f08a-4a18-9cce-69e8a8e94de2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
            "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
          ]
        }
      ],
      "source": [
        "import scispacy\n",
        "import spacy\n",
        "#dfab2temp.drop(['cleaned_str_scispacymd'],axis=1,inplace=True)\n",
        "nlp = spacy.load(\"en_core_sci_md\")\n",
        "\n",
        "list_tokens_labels={}\n",
        "list_tokens_labels2=[]\n",
        "#nlp = spacy.load(\"en_core_sci_md\")\n",
        "for i in range(len(dfab2temp)):\n",
        "  hv_temp_list=[]\n",
        "  text=dfab2temp.iloc[i,0]\n",
        "  doc = nlp(text)\n",
        "  #list_tokens_labels[dfab2temp.iloc[i,-1]]=doc.ents\n",
        "  list_tokens_labels2.append(list(set(doc.ents)))\n",
        "\n",
        "dfab2temp['cleaned_str_scispacymd']= list_tokens_labels2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8PjUJ5gZPWo"
      },
      "outputs": [],
      "source": [
        "list2=dfab2temp['cleaned_str_scispacymd'].tolist()\n",
        "\n",
        "list3=[]\n",
        "for i in list2:\n",
        "  list_t=[]\n",
        "  for j in i:\n",
        "      list_t.append(str(j))\n",
        "  list3.append(list_t)\n",
        "\n",
        "list3a=[]\n",
        "for i in list3:\n",
        "  list_t=''\n",
        "  for j in i:\n",
        "      list_t=list_t+j+' '\n",
        "  list3a.append(list_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_tL3LP1bK60"
      },
      "outputs": [],
      "source": [
        "dfab2temp['cleaned_str_scispacymd2']=list3a"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1=dfab2temp.groupby('cleaned_str_scispacymd2')['url'].count().reset_index(name='count').sort_values(['count'], ascending=False)['cleaned_str_scispacymd2'].tolist()\n",
        "a1=[i for i in a1 if len(i)>0]"
      ],
      "metadata": {
        "id": "7tD4KJE0tLmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2temp2=dfab2temp[dfab2temp['cleaned_str_scispacymd2'].isin(a1)]"
      ],
      "metadata": {
        "id": "OLdAyefmtktJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2temp4=dfab2temp2.copy()\n",
        "dfab2temp4['urls']=dfab2temp4['url'].str.split(' ')\n",
        "dfab2temp5=dfab2temp4.explode('url')\n",
        "dfab2temp5=dfab2temp5[['url','cleaned_str_scispacymd2']]\n",
        "#dfab2temp5.drop_duplicates(inplace=True)\n",
        "dfab2temp5.head()"
      ],
      "metadata": {
        "id": "PErAccihtnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2temp5.to_csv('final_all_syns5_hpo.csv',index=None) # extracted metadata by nlp model for each class stored in csv format"
      ],
      "metadata": {
        "id": "zOadNUPVtr9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Label Information and Creating Finalized Normal OWL File"
      ],
      "metadata": {
        "id": "1C8E2ogSt4U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfab2temp5=pd.read_csv('final_all_syns5_hpo.csv')\n",
        "\n",
        "dfab2temp_c=dfab2temp5.copy()\n",
        "\n",
        "dfab2temp5=dfab2temp5[['url','cleaned_str_scispacymd2']]"
      ],
      "metadata": {
        "id": "UErib1VCt-oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_alln=[]\n",
        "\n",
        "for i in range(len(dfab2temp5)):\n",
        "      #ab=[dfab2temp5.iloc[i,1][1:-1].split(',')]\n",
        "      ab=[dfab2temp5.iloc[i,1].split(',')]\n",
        "      list_alln.extend(ab)\n",
        "\n",
        "dfab2temp5['cleaned_str_scispacymd2']=list_alln\n",
        "\n",
        "dfab2url2=dfab2temp5.explode('cleaned_str_scispacymd2')\n",
        "dfab2url2.head(3)"
      ],
      "metadata": {
        "id": "O34K7SMzxhDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2url2['cleaned_str_scispacymd2']=dfab2url2['cleaned_str_scispacymd2'].str.replace(\"'\",'').str.lstrip()"
      ],
      "metadata": {
        "id": "JcEGEIb9xqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2url2=dfab2url2[dfab2url2['cleaned_str_scispacymd2']!='']\n",
        "dfab2url2.head()"
      ],
      "metadata": {
        "id": "pH7NUn-pyE5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dfab2url2.to_csv('extracted_hpo_ins.csv',index=None)"
      ],
      "metadata": {
        "id": "-lryswBRyMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod3=[]\n",
        "with open(r'hp_8s11feb24_norm2.owl', 'r') as fp:  #normalized owl file by normalizer.groovy\n",
        "    for line in fp:\n",
        "        # remove linebreak from a current name\n",
        "        # linebreak is the last character of each line\n",
        "        #if '0002481' in line :\n",
        "        if 'SubObjectPropertyOf' not in line:\n",
        "           x = line[:-1]\n",
        "\n",
        "        # add current item to the list\n",
        "           mod3.append(x)\n",
        "\n",
        "unique_tokens=list(dfab2url2['cleaned_str_scispacymd2'].unique())\n",
        "\n",
        "dict_hv2={}\n",
        "for i in unique_tokens:\n",
        "      dict_hv2[i]={'nf1':0,'nf2':0,'nf3':0,'nf4':0,'nf5':0}\n",
        "\n",
        "dict_del_nf={}\n",
        "for i in unique_tokens:\n",
        "      dict_del_nf[i]=[]\n",
        "\n",
        "dict_nf1={}\n",
        "for i in unique_tokens:\n",
        "      dict_nf1[i]=[]\n",
        "\n",
        "for i in unique_tokens:\n",
        "    temp_df=dfab2url2[dfab2url2['cleaned_str_scispacymd2']==i]\n",
        "    temp_df_l=temp_df['url'].tolist()\n",
        "    #print(temp_df_l[0])\n",
        "    for j in temp_df_l:\n",
        "      for k in mod3:\n",
        "            if j in k and 'Object' not in k:\n",
        "                     dict_hv2[i]['nf1']+=1\n",
        "                     kj=k[11:-1].split()\n",
        "                     dict_nf1[i].append([kj[0][1:-1],kj[1][1:-1]])\n",
        "\n",
        "            if j in k and 'ObjectIntersectionOf' in k:\n",
        "                     dict_hv2[i]['nf2']+=1\n",
        "                     kall=k[32:].replace(')','').replace('<','').replace('>','').split()\n",
        "                     for kaa in kall:\n",
        "                      if kaa not in dict_del_nf[i]:\n",
        "                        dict_del_nf[i].append(kaa)\n",
        "\n",
        "            if j in k and 'owl:Nothing' in k:\n",
        "                     dict_hv2[i]['nf5']+=1\n",
        "                     #dict_del_nf[i]\n",
        "            if j in k and k[11]!='O' and 'ObjectSomeValuesFrom' in k:\n",
        "                     dict_hv2[i]['nf3']+=1\n",
        "                     if k.split('<')[1][:41] not in dict_del_nf[i]:\n",
        "                         dict_del_nf[i].append(k.split('<')[1][:41])\n",
        "                     if k.split('<')[3][:-3] not in dict_del_nf[i]:\n",
        "                         dict_del_nf[i].append(k.split('<')[3][:-3])\n",
        "            if j in k and k[11]=='O' and 'ObjectSomeValuesFrom' in k:\n",
        "                     dict_hv2[i]['nf4']+=1\n",
        "                     if k[32:-1].split(' ')[1][1:-2] not in dict_del_nf[i]:\n",
        "                         dict_del_nf[i].append(k[32:-1].split(' ')[1][1:-2])\n",
        "                     if k[32:-1].split(' ')[2][1:-1] not in dict_del_nf[i]:\n",
        "                         dict_del_nf[i].append(k[32:-1].split(' ')[2][1:-1])"
      ],
      "metadata": {
        "id": "cWB4iX76yVfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_allb=[]\n",
        "for i in dict_nf1:\n",
        "    for j in dict_nf1[i]:\n",
        "      list_allb.append(j)\n",
        "      #print(j)\n",
        "      #list_allbc.append(j[0])\n",
        "      #list_allbd.append(j[1])\n",
        "\n",
        "list_allbc=[i[0] for i in list_allb]\n",
        "list_allbd=[i[1] for i in list_allb]\n",
        "\n",
        "all_key_nf1=[]\n",
        "for i in dict_nf1:\n",
        "       a1=[i]*len(list(dict_nf1[i]))\n",
        "       for j in a1:\n",
        "             all_key_nf1.append(j)\n",
        "\n",
        "df_nf1_2=pd.DataFrame(all_key_nf1,columns=['cleaned_str_scispacymd2'])\n",
        "df_nf1_2['subclass_c']=list_allbc\n",
        "df_nf1_2['subclass_d']=list_allbd"
      ],
      "metadata": {
        "id": "PILAFCBvynUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_af=[]\n",
        "for i in list(dict_del_nf.values()):\n",
        "  #print(i)\n",
        "  #print('---------')\n",
        "  list_af.append(i)\n",
        "\n",
        "pd.DataFrame(list_af).T\n",
        "\n",
        "all_key=[]\n",
        "for i in dict_del_nf:\n",
        "       a1=[i]*len(list(dict_del_nf[i]))\n",
        "       for j in a1:\n",
        "             all_key.append(j)\n",
        "\n",
        "\n",
        "all_vals=[]\n",
        "for i in dict_del_nf:\n",
        "  list_val=list(dict_del_nf[i])\n",
        "  for j in list_val:\n",
        "    all_vals.append(j)\n",
        "\n",
        "print(len(all_key),len(all_vals))\n",
        "\n",
        "\n",
        "df_nf=pd.DataFrame(all_key)\n",
        "df_nf.columns=['cleaned_str_scispacymd2']\n",
        "df_nf['urls']=all_vals\n",
        "\n",
        "df_nf.head()"
      ],
      "metadata": {
        "id": "WCXruw_kyqHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##optional\n",
        "\n",
        "dfab2url2.rename(columns={'url':'urls'},inplace=True)"
      ],
      "metadata": {
        "id": "J_3kM2FXyxti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_commons=pd.merge(left=dfab2url2,right=df_nf,on=['cleaned_str_scispacymd2','urls'],how='inner').drop_duplicates()\n",
        "#df_commons.head()\n",
        "\n",
        "df_fnf1=pd.merge(left=df_commons,right=df_nf1_2,how='inner',on=['cleaned_str_scispacymd2']).drop_duplicates()\n",
        "df_nf1f=df_fnf1.drop('urls',axis=1).drop_duplicates()\n",
        "\n"
      ],
      "metadata": {
        "id": "zD6KfhBFz5ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfab2temp4=dfab2url2.copy()\n",
        "dfab2temp4['url']=dfab2temp4['url'].str.split(' ')\n",
        "dfab2temp5=dfab2temp4.explode('url')\n"
      ],
      "metadata": {
        "id": "xrDe48l10NSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_axioms=[]\n",
        "#for i in range(len(df_nf1f2)):\n",
        "for i in range(len(dfab2temp5)):\n",
        "  #aa='InstanceOf(<'+df_nf1f2.iloc[i,0]+'> <'+df_nf1f2.iloc[i,1]+'> <'+df_nf1f2.iloc[i,2]+'>)'\n",
        "  aa='InstanceOf(<'+dfab2temp5.iloc[i,1]+'> <'+dfab2temp5.iloc[i,0]+'> <'+dfab2temp5.iloc[i,0]+'>)'\n",
        "  save_axioms.append(aa)\n",
        "#save_axioms[-1]\n",
        "\n",
        "\n",
        "go_or=[]\n",
        "with open(r'hp_8s11feb24_norm2.owl', 'r') as fp:\n",
        "    for line in fp:\n",
        "        # remove linebreak from a current name\n",
        "        # linebreak is the last character of each line\n",
        "        #if '0002481' in line :\n",
        "           x = line[:-1]\n",
        "\n",
        "        # add current item to the list\n",
        "           go_or.append(x)\n",
        "#go_or[-1]\n",
        "\n",
        "#all_axioms_f=go_or+instance_axiom\n",
        "all_axioms_f=go_or+save_axioms\n",
        "all_axiom=list(set(all_axioms_f))\n",
        "print(len(all_axiom))\n",
        "\n",
        "\n",
        "ts='\\n'.join(all_axiom)\n",
        "file = open('HPO_metadata_finalnf1_26Feb_all.owl', 'w') #Final owl file for training\n",
        "file.write(ts)  #141134\n",
        "file.close()"
      ],
      "metadata": {
        "id": "HccWXFkO1doI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test-Train-Split"
      ],
      "metadata": {
        "id": "0QPmrbi844vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH=\"{}.{}\"\n",
        "FILE_NAME=\"HPO_metadata_finalnf1_26Feb_all.owl\"  #name of finalized ontology (.owl)\n",
        "\n"
      ],
      "metadata": {
        "id": "AHpVEKC77RJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file_path = FILE_NAME\n",
        "  #print(SPLIT)\n",
        "file = {}\n",
        "  # count = {}\n",
        "for k in ['train', 'valid', 'test']:\n",
        "\n",
        "    #file[k] = open(SAVE_PATH.format(DATASET, SPLIT, k, \"txt\"), 'w')\n",
        "    file[k] = open(SAVE_PATH.format( k, \"txt\"), 'w')\n",
        "    # count[k] = 0\n",
        "file['train_norm'] = open(SAVE_PATH.format( \"train_norm\", \"owl\"), 'w')\n",
        "  # count['train_norm'] = 0\n",
        "subclass_axioms = []\n",
        "other_axioms = []\n",
        "co=0\n",
        "train = 0\n",
        "valid = 0\n",
        "test = 0\n",
        "classes = {}\n",
        "relations = {}\n",
        "subclass_classes = {}\n",
        "  # First pass: Get all classes in subclass axioms, number of axioms\n",
        "with open(file_path) as f:\n",
        "    for line in f:\n",
        "      co+=1\n",
        "      og_line = line\n",
        "      if line.startswith(\"SubClassOf\"):\n",
        "        line = line.strip()[11:-1]\n",
        "        if not line:\n",
        "          co-=1\n",
        "          continue\n",
        "        if not line.startswith(\"ObjectIntersectionOf(\") and not line.startswith(\"ObjectSomeValuesFrom(\") and line.find(\"ObjectSomeValuesFrom(\") == -1:\n",
        "          # SubClassOf C D\n",
        "          it = line.split(' ')\n",
        "          c = it[0]\n",
        "          d = it[1]\n",
        "          # subclass_axioms.append((og_line, c, d))\n",
        "          if c not in subclass_classes:\n",
        "            subclass_classes[c] = len(subclass_classes)\n",
        "          if d not in subclass_classes:\n",
        "            subclass_classes[d] = len(subclass_classes)\n",
        "\n",
        "train = int(co*0.7)\n",
        "valid = int(co*0.3)\n",
        "test = co - train- valid\n",
        "if test>0:\n",
        "     valid+=test\n",
        "     test=0\n",
        "ex_train = train\n",
        "total_train = train\n",
        "count=0\n",
        "print(\"total\", co)\n",
        "print(\"Expected train, valid, test:\", train, valid, test)\n",
        "\n",
        "  # Second pass: add all relation (rbox) axioms and only add non subclass axioms containing classes in subclass axioms\n",
        "with open(file_path) as f:\n",
        "    for line in f:\n",
        "      og_line = line\n",
        "      if line.startswith('SubObjectPropertyOf'):\n",
        "        line = line.strip()[20:-1]\n",
        "        if line.startswith('ObjectPropertyChain'):\n",
        "          line_chain = line.strip()[20:-1]\n",
        "          line1 = line.split(\")\")\n",
        "          line10 = line1[0].split()\n",
        "          if len(line10) < 2:\n",
        "            continue\n",
        "          r1 = line10[0]\n",
        "          r2 = line10[1]\n",
        "          r3 = line1[1]\n",
        "          # if train and (r1 not in relations or r2 not in relations or r3 not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(r1 + ' ' + r2 + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "            # if r1 not in relations:\n",
        "            #   relations[r1] = len(relations)\n",
        "            # if r2 not in relations:\n",
        "            #   relations[r2] = len(relations)\n",
        "            # if r3 not in relations:\n",
        "            #   relations[r3] = len(relations)\n",
        "          # else:\n",
        "            # other_axioms.append((og_line, r1, r2, r3))\n",
        "        else:\n",
        "          # print(\"Inside sub obj prop\")\n",
        "          it = line.split(' ')\n",
        "          r1 = it[0]\n",
        "          r2 = it[1]\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(r1 + ' ' + r2 + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          # other_axioms.append((og_line, r1, r2, \"\"))\n",
        "        continue\n",
        "      line = line.strip()[11:-1]\n",
        "#           print(line)\n",
        "      if not line:\n",
        "        print(og_line)\n",
        "        continue\n",
        "      if line.startswith('ObjectIntersectionOf('):\n",
        "        # C and D SubClassOf E\n",
        "        it = line.split(' ')\n",
        "        c = it[0][21:]\n",
        "        d = it[1][:-1]\n",
        "        e = it[2]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or e in subclass_classes):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if e in subclass_classes:\n",
        "            subclass_classes.pop(e)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if e not in classes:\n",
        "            classes[e] = len(classes)\n",
        "        else:\n",
        "          other_axioms.append((og_line, c, d, e))\n",
        "      elif line.startswith('ObjectSomeValuesFrom('):\n",
        "        # R some C SubClassOf D\n",
        "        it = line.split(' ')\n",
        "        r = it[0][21:]\n",
        "        c = it[1][:-1]\n",
        "        d = it[2]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or r not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if r not in relations:\n",
        "            relations[r] = len(relations)\n",
        "        else:\n",
        "          other_axioms.append((og_line, r, c, d))\n",
        "      elif line.find('ObjectSomeValuesFrom') != -1:\n",
        "        # C SubClassOf R some D\n",
        "        it = line.split(' ')\n",
        "        c = it[0]\n",
        "        r = it[1][21:]\n",
        "        d = it[2][:-1]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or r not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if r not in relations:\n",
        "            relations[r] = len(relations)\n",
        "        else:\n",
        "          other_axioms.append((og_line, r, c, d))\n",
        "      else:\n",
        "        # C SubClassOf D\n",
        "        it = line.split(' ')\n",
        "        c = it[0]\n",
        "        d = it[1]\n",
        "        subclass_axioms.append((og_line, c, d))\n",
        "print(\"Added to train from other axioms\", ex_train-train, count)\n",
        "print(count+len(subclass_axioms)+len(other_axioms), len(subclass_axioms), len(other_axioms))\n",
        "print(\"Classes not added\", len([k for k in subclass_classes if k not in classes]), len(subclass_classes))\n",
        "ex_train = train\n",
        "# Second pass\n",
        "temp = []\n",
        "for tup in subclass_axioms:\n",
        "    line, c, d = tup\n",
        "    if train and (c in subclass_classes or d in subclass_classes):\n",
        "#       print(line)\n",
        "      file['train_norm'].write(line)\n",
        "      file['train'].write(c + ' ' + d + '\\n')\n",
        "      count+=1\n",
        "      train-=1\n",
        "      if c in subclass_classes:\n",
        "        subclass_classes.pop(c)\n",
        "      if d in subclass_classes:\n",
        "        subclass_classes.pop(d)\n",
        "      if c not in classes:\n",
        "        classes[c] = len(classes)\n",
        "      if d not in classes:\n",
        "        classes[d] = len(classes)\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "subclass_axioms = temp\n",
        "print(count+len(subclass_axioms)+len(other_axioms), len(subclass_axioms), len(other_axioms))\n",
        "print(\"Added to train from subclass axioms\", ex_train-train)\n",
        "ex_train = train\n",
        "\n",
        "print(\"remaining subclass axioms and other axioms\", len(subclass_axioms), len(other_axioms))\n",
        "print(\"train, valid, test\", train, valid, test)\n",
        "\n",
        "if len(subclass_axioms) < test+valid:\n",
        "    # recalculate test, validation, training sample counts\n",
        "    c = len(subclass_axioms)\n",
        "    #valid = int(c*0.66)\n",
        "    valid = int(c)\n",
        "    #test = c - valid\n",
        "   # if test>0:\n",
        "    # valid+=test\n",
        "     #test=0\n",
        "    train = max(0, min(train, int(c*0.7/0.3) - (total_train-train)))\n",
        "ex_train = train\n",
        "\n",
        "print(\"Train, valid, test left\", train, valid, test)\n",
        "\n",
        "temp = []\n",
        "while test and subclass_axioms != []:\n",
        "    line, c, d = subclass_axioms.pop()\n",
        "    if c not in subclass_classes and d not in subclass_classes:\n",
        "      file['test'].write(c + ' ' + d + '\\n')\n",
        "      test-=1\n",
        "      count+=1\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "while valid and subclass_axioms != []:\n",
        "    line, c, d =  subclass_axioms.pop()\n",
        "    if c not in subclass_classes and d not in subclass_classes:\n",
        "      file['valid'].write(c + ' ' + d + '\\n')\n",
        "      valid-=1\n",
        "      count+=1\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "subclass_axioms.extend(temp)\n",
        "while train and other_axioms != []:\n",
        "    line, r, c, d = other_axioms.pop()\n",
        "#     if c in classes and d in classes:\n",
        "    file['train'].write(c + ' ' + d + '\\n')\n",
        "    file['train_norm'].write(line)\n",
        "    count+=1\n",
        "    train-=1\n",
        "print(\"added to train from other axioms\", ex_train-train)\n",
        "ex_train = train\n",
        "while train and subclass_axioms != []:\n",
        "    line, c, d =  subclass_axioms.pop()\n",
        "#     if c in classes or d in classes:\n",
        "    file['train'].write(c + ' ' + d + '\\n')\n",
        "    file['train_norm'].write(line)\n",
        "    count+=1\n",
        "    train -=1\n",
        "print(\"added to train from subclass axioms\", ex_train-train)\n",
        "for k in ['train', 'valid', 'test']:\n",
        "    file[k].close()\n",
        "print(\"Total:\", count)\n",
        "print(\"left\", len(subclass_axioms), len(other_axioms))\n",
        "print(train, test, valid)"
      ],
      "metadata": {
        "id": "A8Yw2SDT46vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the above code gives two main file- train.txt, valid.txt, test.txt, which can be used in Meta-N-Ball Model."
      ],
      "metadata": {
        "id": "ickovEQamHNk"
      }
    }
  ]
}